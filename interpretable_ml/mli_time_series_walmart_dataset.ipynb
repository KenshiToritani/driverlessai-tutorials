{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis on a Driverless AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from h2oai_client import Client, ModelParameters, InterpretParameters\n",
    "from sklearn.model_selection import train_test_split\n",
    "from subprocess import Popen, PIPE\n",
    "from dateutil.parser import parse\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Walmart Weekly Sales Data\n",
    "\n",
    "For this notebook we are using the Kaggle Walmart Sales Forecasting challenge. You can download the data here:\n",
    "https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data#\n",
    "\n",
    "Here is a description of the features in the dataset:\n",
    "```\n",
    "Store - the store number\n",
    "Date - the week\n",
    "Dept - the department number\n",
    "Weekly_Sales -  sales for the given department in the given store\n",
    "IsHoliday - whether the week is a special holiday week\n",
    "Temperature - average temperature in the region\n",
    "Fuel_Price - cost of fuel in the region\n",
    "MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. \n",
    "CPI - the consumer price index\n",
    "Unemployment - the unemployment rate\n",
    "IsHoliday - whether the week is a special holiday week\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write train and test set to disk for later use in DAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/walmart/train.csv\"\n",
    "walmart_df = pd.read_csv(train_path)\n",
    "time1_path = \"data/walmart/walmart_train.csv\"\n",
    "time2_path = \"data/walmart/walmart_test_with_actuals.csv\"\n",
    "split_date = \"2011-07-01\"\n",
    "\n",
    "# Split data by time\n",
    "time1_pd = walmart_df.loc[walmart_df.Date < split_date] \n",
    "time2_pd = walmart_df.loc[walmart_df.Date >= split_date]\n",
    "\n",
    "# Generate the train and test datasets ...\n",
    "if not (os.path.isfile(time1_path) and os.path.isfile(time2_path)):\n",
    "    time1_pd.to_csv(time1_path, index=False)\n",
    "    time2_pd.to_csv(time2_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick description about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the `Weekly_sales` column\n",
    "time2_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = time2_pd.groupby(['Store', 'Dept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped.get_group((18, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2_pd['Weekly_Sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without the `Weekly_sales` column\n",
    "time3_pd = time2_pd.loc[:, time2_pd.columns != 'Weekly_Sales']\n",
    "time3_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time3_pd.to_csv(\"data/walmart/walmart_test_without_actuals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Driverless AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = 'localhost:12345'\n",
    "username = 'pramit'\n",
    "password = 'pramit'\n",
    "h2oai = Client(address = 'http://' + ip, username = username, password = password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about the DAI Client:\n",
    "print(h2oai.address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to Driverless AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since DAI needs absolute path ...\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "train_path_dai = f\"{current_directory}/data/walmart/walmart_train.csv\"\n",
    "test_path_dai_with_actuals = f\"{current_directory}/data/walmart/walmart_test_with_actuals.csv\"\n",
    "test_path_dai_without_actuals = f\"{current_directory}/data/walmart/walmart_test_without_actuals.csv\"\n",
    "\n",
    "# Upload datasets\n",
    "train = h2oai.upload_dataset_sync(train_path_dai)\n",
    "validation = h2oai.upload_dataset_sync(test_path_dai_with_actuals)\n",
    "\n",
    "test = validation\n",
    "#test = h2oai.upload_dataset_sync(test_path_dai_without_actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parameters for Driverless AI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters you want to pass to DAI\n",
    "dataset_key=train.key # Dataset to use for DAI\n",
    "validset_key='' # Validation set to use for DAI (Note, we are not using one for this experiment)\n",
    "testset_key=test.key # Test set to use for DAI\n",
    "target=\"Weekly_Sales\" # Target column for DAI\n",
    "# dropped_cols=[\"sample_weight\"] #List of columns to drop. In this case we are dropping 'sample_weight'\n",
    "weight_col=None # The column that indicates the per row observation weights. \n",
    "                # If None, each row will have an observation weight of 1\n",
    "fold_col=None # The column that indicates the fold. If None, the folds will be determined by DAI\n",
    "time_col='Date' # Time Column: The column that provides a time order, if applicable.\n",
    "                  # if [AUTO], Driverless AI will auto-detect a potential time order\n",
    "                  # if [OFF], auto-detection is disabled\n",
    "is_time_series=True # Whether or not the experiment is a time series problem\n",
    "is_classification=False # Inform DAI if the problem type is a classification (binomial/multinomial) \n",
    "                    # or not (regression)\n",
    "enable_gpus=True # Whether or not to enable GPUs\n",
    "seed=1234 # Use seed for reproducibility\n",
    "scorer_str='R2' # Set evaluation metric. In this case we are interested in optimizing R-squared\n",
    "accuracy=1 # Accuracy setting for experiment (One of the 3 knobs you see in the DAI UI)\n",
    "time=1 # Time setting for experiment (One of the 3 knobs you see in the DAI UI)\n",
    "interpretability=2 # Interpretability setting for experiment (One of the 3 knobs you see in the DAI UI)\n",
    "config_overrides=None # Extra parameters that can be passed in TOML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information on the experiment settings, refer to the [Experiment Settings](http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/launching.html#experimentsettings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set time-series-specific settings\n",
    "Here we choose the prediction horizon for our final model from the time series recipe. The horizon is a fixed window from which the time series model can output a valid prediction. The model can output intermediate predictions. Intermediate meaning the time between training+gap until the end of the horizon. To see more detail on how to choose and set the horizon please see:\n",
    "http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/time-series-use-case.html\n",
    "\n",
    "Since the horizon is fixed, it may be necessary to try multiple models with various horizons to choose the right model based on a balance of retraining frequency and\n",
    "performance.\n",
    "\n",
    "The gap period option is to simulate during training and validation, the time it takes to record or gather data. No predictions or data will be used from this gap period.\n",
    "\n",
    "The numeric values here are in weeks, in accordance with the Walmart dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: horizon_in_weeks is how many weeks the model can predict out to. \n",
    "#       Intermediate weeks ARE predicted by the model\n",
    "horizon_in_weeks = 39\n",
    "\n",
    "# Gap between train dataset and when predictions will start\n",
    "num_gap_periods = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch experiment\n",
    "Launch the experiment using the accuracy, time, and interpretability settings DAI suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your experiment is already done\n",
    "#experiment = h2oai.get_model_job('sapucota').entity \n",
    "\n",
    "experiment = h2oai.start_experiment_sync(\n",
    "    #Datasets\n",
    "    dataset_key=train.key, \n",
    "    validset_key = validset_key,\n",
    "    testset_key=testset_key, \n",
    "    \n",
    "    #Columns\n",
    "    target_col=target,\n",
    "    #cols_to_drop=dropped_cols,\n",
    "    weight_col=weight_col,\n",
    "    fold_col=fold_col,\n",
    "    orig_time_col=time_col,\n",
    "    time_col=time_col,\n",
    "    \n",
    "    #Parameters\n",
    "    is_classification=is_classification,\n",
    "    enable_gpus=enable_gpus,\n",
    "    seed=seed,\n",
    "    accuracy=accuracy, #DAI suggested for accuracy\n",
    "    time=time, #DAI suggested for time\n",
    "    interpretability=interpretability, #DAI suggested for interpretability\n",
    "    scorer=scorer_str,\n",
    "    is_timeseries=is_time_series,\n",
    "    #Time series parameters that need to be set but are not used as they are set to None/False\n",
    "    time_groups_columns=['Date','Store', 'Dept'],   # The features used to group time series data together, \n",
    "                                                    # Each row would have a unique combination of these features\n",
    "    time_period_in_seconds=3600*24*7*horizon_in_weeks,\n",
    "    num_gap_periods=num_gap_periods,\n",
    "    num_prediction_periods=1, # or 39 and dont * time_period_in_seconds by 39\n",
    "    #Extra parameters that can be passed in TOML format\n",
    "    config_overrides=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the final model score for the validation and test datasets. When feature engineering is complete, an ensemble model can be built depending on the accuracy setting. The experiment object also contains the score on the validation and test data for this ensemble model. In this case, the validation score is the score on the training cross-validation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Model Score on Validation Data: \" + str(round(experiment.valid_score, 3)))\n",
    "print(\"Final Model Score on Test Data: \" + str(round(experiment.test_score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable importance for Driverless AI experiment\n",
    "The table outputted below shows the feature name, its relative importance, and a description. Some features will be engineered by Driverless AI and some can be the original feature.\n",
    "\n",
    "The DAI Time Series recipe uses time based transformations such as Lags which are past values of a numeric features. You may also see EWMA, this is the Exponentionally Weighted Moving Average transformation on historical values of a particular numeric feature. This transformation calculates the change in a variable over a period of time and decays that change over a longer period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.summary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Summary\n",
    "import subprocess\n",
    "summary_path = h2oai.download(src_path=experiment.summary_path, dest_dir=\".\")\n",
    "dir_path = \"./h2oai_experiment_summary_\" + experiment.key\n",
    "subprocess.call(['unzip', '-o', summary_path, '-d', dir_path], shell=False)\n",
    "\n",
    "#View Features\n",
    "features = pd.read_table(dir_path + \"/features.txt\", sep=',', skipinitialspace=True)\n",
    "features.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up scoring package from Driverless AI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2oai.download(experiment.scoring_pipeline_path, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash unzip scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scoring module\n",
    "!pip install scoring-pipeline/scoring_h2oai_experiment_*.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Name:\n",
    "print(f\"Experiment Name: {experiment.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_h2oai_experiment_wesecico import Scorer #Make sure to add experiment name to  \n",
    "                                                     #import scoring_h2oai_experiment_* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Create a singleton Scorer instance.\n",
    "#For optimal performance, create a Scorer instance once, and call score() or score_batch() multiple times.\n",
    "scorer = Scorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1_pd[\"predict\"] = scorer.score_batch(time1_pd)\n",
    "time2_pd = time2_pd.reset_index(drop=True)\n",
    "time2_pd[\"predict\"] = scorer.score_batch(time2_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Shapley Values\n",
    "Shapley values show the contribution of engineered features to the predicted weekly sales generated by the model.\n",
    "With Shapley values you can break down the components of a prediction and attribute values to specfic features. Please note, in some cases the model has a \"link function\" that yet to be applied to make the sum of the Shapley contributions equal to the prediction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.get_column_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test= time1_pd.append(time2_pd)\n",
    "train_and_test = train_and_test.reset_index(drop=True)\n",
    "shapley = scorer.score_batch(train_and_test, pred_contribs=True, fast_approx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley.columns = [x.replace('contrib_','',1) for x in shapley.columns]\n",
    "print(shapley.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Here we look at the overall model performance in test and train. We also show the model horizon window in red to illustrate the performance when the model is generating predictions beyond the horizon.\n",
    "We prefer to use R-squared as the performance metric since the groups of Store and Department weekly sales are on vastly different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_and_test[train_and_test['predict'].isnull() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def r2_rmse( g ):\n",
    "    r2 = r2_score( g['Weekly_Sales'], g['predict'] )\n",
    "    rmse = np.sqrt( mean_squared_error( g['Weekly_Sales'], g['predict'] ) )\n",
    "    return pd.Series( dict(  r2 = r2, rmse = rmse ) )\n",
    "\n",
    "metrics_ts = train_and_test.groupby( 'Date' ).apply( r2_rmse )\n",
    "# metrics_ts.index = [parse(x) for x in metrics_ts.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Time Series Plot\n",
    "This would be a useful plot to compare R2 over time between different DAI time series models each with different prediction horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ts['r2'].plot(figsize=(20,10), title=(\"Training dataset in Green. \\n\n",
    "                                              Test data with model prediction horizon in Red\"))\n",
    "\n",
    "\n",
    "test_window_start = str(parse(time2_pd[\"Date\"].min()) + datetime.timedelta(weeks=num_gap_periods))\n",
    "test_window_end = str(parse(test_window_start) + datetime.timedelta(weeks=horizon_in_weeks))\n",
    "\n",
    "\n",
    "plt.axvspan(time2_pd[\"Date\"].min(), test_window_end, facecolor='r', alpha=0.1)\n",
    "plt.axvspan(time1_pd[\"Date\"].min(), test_window_start, facecolor='g', alpha=0.1)\n",
    "plt.suptitle(\"R2 Time Series Plot\",  fontsize=21, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst and Best Groups\n",
    "Here we generate the best and worst groups by R2. We filter out groups that have some missing data. We only calculate R2 within the valid test horizon window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_count = train_and_test.groupby([\"Store\",\"Dept\"]).size().mean()\n",
    "print(\"average count: \" + str(avg_count))\n",
    "train_and_test_filtered = train_and_test.groupby([\"Store\",\"Dept\"]).filter(lambda x: len(x) > 0.8 * avg_count)\n",
    "train_and_test_filtered = train_and_test_filtered.loc[(train_and_test_filtered.Date < test_window_end) &\n",
    "                                                     (train_and_test_filtered.Date > test_window_start)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_r2s = train_and_test_filtered.groupby([\"Store\",\"Dept\"]).apply( r2_rmse ).sort_values(\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_r2s.head()) # worst groups\n",
    "print(grouped_r2s.tail()) # best groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Group\n",
    "store_num = 26\n",
    "dept_num = 12\n",
    "date_selection = '2012-02-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_df = train_and_test[(train_and_test.Store == store_num) & (train_and_test.Dept == dept_num)][[\"Date\",\"Weekly_Sales\",\"predict\"]]\n",
    "plot_df[\"Date\"] = plot_df[\"Date\"].apply(lambda x: parse(x))\n",
    "plot_df = plot_df.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.plot(figsize=(20,10), title=(\"Training dataset in Green. Test data with model prediction horizon in Red. Date selection in Yellow\"))\n",
    "\n",
    "plt.axvspan(time2_pd[\"Date\"].min(), test_window_end, facecolor='r', alpha=0.1)\n",
    "plt.axvspan(time1_pd[\"Date\"].min(), test_window_start, facecolor='g', alpha=0.1)\n",
    "plt.axvline(x=date_selection, color='y')\n",
    "plt.suptitle(\"Actual vs. Predicted\", fontsize=21, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Shapley\n",
    "This is a global vs local Shapley plot, with global being the average Shapley values for all of the predictions in the selected group and local being the Shapley value for that specific prediction. Looking at this plot can give clues as to which features contributed to the error in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_vals_group = shapley.loc[(train_and_test.Store==store_num) & (train_and_test.Dept==dept_num),:]\n",
    "shap_vals_timestamp = shapley.loc[(train_and_test.Store==store_num) \n",
    "                                  & (train_and_test.Dept==dept_num)\n",
    "                                  & (train_and_test.Date==date_selection),:]\n",
    "shap_vals = shap_vals_group.mean()\n",
    "shap_vals = pd.concat([pd.DataFrame(shap_vals), shap_vals_timestamp.transpose()], axis=1, ignore_index=True)\n",
    "shap_vals = shap_vals.sort_values(by=0)\n",
    "bias = shap_vals.loc[\"bias\",0]\n",
    "shap_vals = shap_vals.drop(\"bias\",axis=0)\n",
    "shap_vals.columns = [\"Global (Group)\", \"Local (Timestamp)\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = FuncFormatter(lambda x, y:  str(round(float(x) + bias)))\n",
    "ax = shap_vals.plot.barh(figsize=(8,30), fontsize=10, colormap=\"Set1\")\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook should get you started with all you need to diagnose and debug time series models from DAI.  Try different horizons during training and compare the model's R2 over time to pick the best horizon for your use case.  Use the actual vs prediction plots to do detailed debugging. Find some interesting dates to examine and use the Shapley plots to see how the features impacted the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
